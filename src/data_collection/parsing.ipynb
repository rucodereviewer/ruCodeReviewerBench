{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "access_token = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_forked_repositories(owner, repository, token=access_token):\n",
    "    forks_url = f\"https://api.github.com/repos/{owner}/{repository}/forks\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(forks_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch forks for {repository}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def get_repos():\n",
    "    forks_data = []\n",
    "\n",
    "    langs = [\"java\", \"go\", \"python\", \"scala\"]\n",
    "    for lang in langs:\n",
    "        for i in range(1, 5 + 1):\n",
    "            owner = 'central-university-dev'\n",
    "            repository = f\"2024-{i}-{lang}-backend-academy-2024-{lang}-template\"\n",
    "            \n",
    "            forks = get_forked_repositories(owner, repository)\n",
    "            \n",
    "            for fork in forks:\n",
    "                repo_data = {\n",
    "                    'original_repository': repository,\n",
    "                    'forked_repository': fork['full_name'],\n",
    "                }\n",
    "                forks_data.append(repo_data)\n",
    "    \n",
    "    df = pd.DataFrame(forks_data)\n",
    "    df.to_csv('data/repos_with_cq_gate.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GitHubRepoParser:\n",
    "    def __init__(self, csv_file, extensions_json):\n",
    "        self.csv_file = csv_file\n",
    "        self.extensions_json = extensions_json\n",
    "        self.language_mapping = self.load_language_mapping()\n",
    "        self.repos = self.load_repos()\n",
    "        self.access_token = access_token\n",
    "        self.num_req = 0\n",
    "\n",
    "    def load_language_mapping(self):\n",
    "        with open(self.extensions_json, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def load_repos(self):\n",
    "        return pd.read_csv(self.csv_file)\n",
    "\n",
    "    def generate_unique_id(self, diff_hunk):\n",
    "        return hashlib.md5(diff_hunk.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def get_with_retry(self, url, headers):\n",
    "        \"\"\"Function to handle HTTP GET requests with retry on 403 error.\"\"\"\n",
    "        while True:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 403:\n",
    "                print(f\"403 error encountered. Waiting 15 minutes before retrying: {url}\")\n",
    "                time.sleep(900)\n",
    "            else:\n",
    "                return response\n",
    "\n",
    "    def get_all_pages(self, url, headers):\n",
    "        all_items = []\n",
    "        while url:\n",
    "            response = self.get_with_retry(url, headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                items = response.json()\n",
    "                all_items.extend(items)\n",
    "                if 'next' in response.links:\n",
    "                    url = response.links['next']['url']\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Failed to fetch page: {response.status_code} {url}\")\n",
    "                break\n",
    "        return all_items\n",
    "    \n",
    "    def get_commits_for_pr(self, repo, pull_number):\n",
    "        commits_url = f\"https://api.github.com/repos/{repo}/pulls/{pull_number}/commits\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {self.access_token}\",\n",
    "            \"Accept\": \"application/vnd.github.v3+json\"\n",
    "        }\n",
    "        \n",
    "        return self.get_all_pages(commits_url, headers)\n",
    "\n",
    "    def get_diff_between_commits(self, repo, base, head):\n",
    "        compare_url = f\"https://api.github.com/repos/{repo}/compare/{base}...{head}\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {self.access_token}\",\n",
    "            \"Accept\": \"application/vnd.github.v3+json\"\n",
    "        }\n",
    "\n",
    "        response = self.get_with_retry(compare_url, headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Failed to compare commits: {base}...{head} with status {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def get_full_file_diffs_in_pr(self, repo, pull_number):\n",
    "        commits = self.get_commits_for_pr(repo, pull_number)\n",
    "        commit_diffs = []\n",
    "\n",
    "        if not commits or len(commits) < 2:\n",
    "            print(f\"No enough commits to process for PR {pull_number}\")\n",
    "            return commit_diffs\n",
    "\n",
    "        for i in range(len(commits)):\n",
    "            for j in range(i + 1, len(commits)):\n",
    "                base_commit_sha = commits[i]['sha']\n",
    "                head_commit_sha = commits[j]['sha']\n",
    "                \n",
    "                diff_data = self.get_diff_between_commits(repo, base_commit_sha, head_commit_sha)\n",
    "                file_diffs = {}\n",
    "\n",
    "                if diff_data and 'files' in diff_data:\n",
    "                    for file_info in diff_data['files']:\n",
    "                        if 'patch' in file_info:\n",
    "                            file_diffs[file_info['filename']] = file_info['patch']\n",
    "                            \n",
    "                            if file_info.get('status') == 'renamed':\n",
    "                                file_diffs[file_info['previous_filename']] = file_info['patch']\n",
    "\n",
    "                commit_diffs.append((head_commit_sha, file_diffs))\n",
    "        \n",
    "        return commit_diffs\n",
    "\n",
    "    def get_pull_request_review_comments(self, repo, pull_number, original_repo):\n",
    "        pr_comments_url = f\"https://api.github.com/repos/{repo}/pulls/{pull_number}/comments\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {self.access_token}\",\n",
    "            \"Accept\": \"application/vnd.github.v3.full+json\"\n",
    "        }\n",
    "\n",
    "        comments = self.get_all_pages(pr_comments_url, headers)\n",
    "        \n",
    "        if comments:\n",
    "            commit_diffs = self.get_full_file_diffs_in_pr(repo, pull_number)\n",
    "\n",
    "            df_dict = defaultdict(list)\n",
    "            for comment in comments:\n",
    "                _, extension = comment[\"path\"].rsplit('.', 1) if '.' in comment[\"path\"] else (None, None)\n",
    "                language = self.language_mapping.get(f\".{extension}\", \"Unk\")\n",
    "\n",
    "                diff_id = self.generate_unique_id(comment[\"diff_hunk\"])\n",
    "                full_diff = \"\"\n",
    "\n",
    "                diff = comment[\"diff_hunk\"]\n",
    "                \n",
    "                for commit_sha, file_diffs in commit_diffs:\n",
    "                    for file_path in file_diffs.values():\n",
    "                        if diff in file_path:\n",
    "                            full_diff = file_path\n",
    "\n",
    "\n",
    "                full_diff_id = self.generate_unique_id(full_diff)\n",
    "\n",
    "                df_dict['diff'].append(comment[\"diff_hunk\"])\n",
    "                df_dict['diff_id'].append(diff_id)\n",
    "                df_dict['start_line'].append(comment[\"start_line\"] if comment[\"start_line\"] else -1)\n",
    "                df_dict['end_line'].append(comment['line'] if comment[\"line\"] else -1)\n",
    "                df_dict['original_start_line'].append(comment[\"original_start_line\"] if comment[\"original_start_line\"] else -1)\n",
    "                df_dict['original_end_line'].append(comment['original_line'] if comment[\"original_line\"] else -1)\n",
    "                df_dict['full_diff'].append(full_diff)\n",
    "                df_dict['full_diff_id'].append(full_diff_id)\n",
    "                df_dict['message'].append(comment[\"body\"])\n",
    "                df_dict['file'].append(comment[\"path\"])\n",
    "                df_dict['comment_url'].append(comment[\"html_url\"])\n",
    "                df_dict['language'].append(language)\n",
    "                df_dict['author_association'].append(comment[\"author_association\"])\n",
    "                df_dict['repo_name'].append(repo)\n",
    "                df_dict['original_repo_name'].append(original_repo)\n",
    "\n",
    "            return pd.DataFrame(df_dict)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_pull_info(self, repo):\n",
    "        prs_url = f\"https://api.github.com/repos/{repo}/pulls\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {self.access_token}\",\n",
    "            \"Accept\": \"application/vnd.github.v3+json\"\n",
    "        }\n",
    "\n",
    "        response = self.get_with_retry(prs_url, headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            pulls = response.json()\n",
    "            return pulls\n",
    "        else:\n",
    "            print(f\"Failed to fetch pull requests: {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "    def get_commit_info(self, repo, commit_sha):\n",
    "        url = f\"https://api.github.com/repos/{repo}/commits/{commit_sha}\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {self.access_token}\",\n",
    "            \"Accept\": \"application/vnd.github.v3+json\"\n",
    "        }\n",
    "        response = self.get_with_retry(url, headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Failed to get commit info: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def process_repo(self, row):\n",
    "        try:\n",
    "            repo = row['forked_repository']\n",
    "            original_repo = row['original_repository']\n",
    "            \n",
    "            pulls_info = self.get_pull_info(repo)\n",
    "            pull_comments_list = []\n",
    "            \n",
    "            for pull_info in pulls_info:\n",
    "                pull_comments = self.get_pull_request_review_comments(repo, pull_info['number'], original_repo)\n",
    "                if pull_comments is not None:\n",
    "                    pull_comments_list.append(pull_comments)\n",
    "            \n",
    "            return pull_comments_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing repo: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def parse_and_save_repos_in_batches(self, batch_size=30):\n",
    "        num_batches = (len(self.repos) // batch_size) + (len(self.repos) % batch_size != 0)\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            batch_repos = self.repos.iloc[batch * batch_size : (batch + 1) * batch_size]\n",
    "            \n",
    "            result = []\n",
    "            with concurrent.futures.ThreadPoolExecutor(10) as executor:\n",
    "                futures = [executor.submit(self.process_repo, row) for _, row in batch_repos.iterrows()]\n",
    "\n",
    "                for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures),\n",
    "                                   desc=f\"Processing batch {batch+1}/{num_batches}\"):\n",
    "                    result.extend(future.result())\n",
    "\n",
    "            result = pd.concat(result, ignore_index=True)\n",
    "            result.to_csv(f\"repos_batch_{batch+1}.csv\", index=False)\n",
    "\n",
    "            print(f\"Batch {batch+1} processed and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
